{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convergencia Tipo X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import sys, os\n",
    "\n",
    "from matplotlib.patches import Ellipse\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import sys, os; sys.path.append(os.path.dirname(os.getcwd()))\n",
    "from pyfrechet.metric_spaces import MetricData, LogCholesky, spd_to_log_chol, log_chol_to_spd\n",
    "from pyfrechet.regression.bagged_regressor import BaggedRegressor\n",
    "from pyfrechet.regression.trees import Tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pyfrechet.metric_spaces import MetricData, LogEuclidean, CustomAffineInvariant, CustomLogEuclidean, AffineInvariant, LogCholesky, log_chol_to_spd, spd_to_log_chol\n",
    "\n",
    "from typing import Union\n",
    "import random\n",
    "\n",
    "from scipy.stats import wishart\n",
    "from scipy.special import digamma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_spd_matrix(q_array, limits_unif = 30, seed = 1):\n",
    "    \"\"\"Generate a random q x q symmetric positive definite (SPD) matrix.\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    q_array = np.array(q_array, dtype = int)\n",
    "    # Ensure the matrices are symmetric positive definite\n",
    "    mat = [(np.random.rand(q_array[i], q_array[i])-1/2)*limits_unif for i in range(len(q_array))]\n",
    "    return [np.dot(mat[i], mat[i].T) for i in range(len(q_array))]\n",
    "\n",
    "def Sigma_t(t_array, Sigma_array):\n",
    "    \"\"\"Provides an array with the matrices given by a regression model that interpolates between four matrices.\"\"\"  \n",
    "    \"\"\"The regression starts with Sigma_1 and then goes to Sigma_2 and Sigma_3 and ends in Sigma_4.\"\"\"\n",
    "    \n",
    "    # Define time intervals for interpolation\n",
    "    t_array = np.array(t_array)\n",
    "    t_array = t_array[:, None, None]\n",
    "\n",
    "    # Return the interpolated matrices\n",
    "    return np.where(t_array < 0.5, np.cos(np.pi*t_array)**2 * Sigma_array[0] + (1 - np.cos(np.pi*(1-t_array))**2) * Sigma_array[1], 0) + np.where(t_array >= 0.5, (1 - np.cos(np.pi*t_array)**2) * Sigma_array[1] + np.cos(np.pi*(1-t_array))**2 * Sigma_array[2], 0)\n",
    "\n",
    "# Define the matrices to interpolate\n",
    "Sigma_1=np.array([[1, -0.6],\n",
    "                  [-0.6, 0.5]])\n",
    "Sigma_2=np.array([[1, 0],\n",
    "                  [0, 1]])\n",
    "Sigma_3=np.array([[0.5, 0.4],\n",
    "                  [0.4, 1]])\n",
    "\n",
    "Sigmas = (Sigma_1, Sigma_2, Sigma_3)\n",
    "\n",
    "def sim_regression_matrices(Sigmas: tuple,\n",
    "                            t: np.array,\n",
    "                            df: int=2):\n",
    "    t = np.array(t)\n",
    "    \n",
    "    #Simulate the time for regression (sample_t) and the true time (true_t)\n",
    "    q = Sigmas[0].shape[0]\n",
    "\n",
    "    c_dq = 2 * np.exp((1 / q) * sum( digamma((df - np.arange(1, q + 1) + 1 ) / 2) ))\n",
    "    sigma_t = Sigma_t(t, Sigmas)\n",
    "    sample_Y = [wishart( df=df, scale = sigma_t[k] / c_dq ).rvs( size=1 ) for k in range(t.shape[0])]\n",
    "    return {'t': t, 'y': sample_Y}\n",
    "\n",
    "def plot_ellipse(mat: np.ndarray, ax, \n",
    "                 xy: tuple=(0,0),\n",
    "                 scale_factor=1,\n",
    "                 edgecolor='red',\n",
    "                 facecolor='None',\n",
    "                 linewidth=2,\n",
    "                 alpha=1):\n",
    "    eigenvalues, eigenvectors = np.linalg.eig(mat)\n",
    "    theta = np.degrees(np.arctan2(*eigenvectors[:, 0][::-1]))\n",
    "    ellipse = Ellipse(xy=xy,\n",
    "                  width=scale_factor*np.sqrt(eigenvalues[0]),\n",
    "                  height=scale_factor*np.sqrt(eigenvalues[1]),\n",
    "                  angle=theta,\n",
    "                  edgecolor=edgecolor,\n",
    "                  facecolor=facecolor,\n",
    "                  lw=linewidth,\n",
    "                  alpha=alpha)\n",
    "    ax.add_patch(ellipse)\n",
    "\n",
    "\n",
    "def plot_OOB_balls_SPD( predictions: np.ndarray,\n",
    "                        indices_to_plot: list[int],\n",
    "                        Ralpha: float,\n",
    "                        ax,\n",
    "                        alpha: float = 0.05,\n",
    "                        reference: Union[np.ndarray, None]=None,\n",
    "                        scale_factor: float=1/10,\n",
    "                        xy_factor: float=50,\n",
    "                        df: int=5,\n",
    "                        MC_samples: int=100,\n",
    "                        edge_color='deepskyblue',\n",
    "                        dist : str = 'LC',\n",
    "                        limits_unif : int = 30\n",
    "                        ) -> None:\n",
    "    index_to_plot = 1\n",
    "    if dist == 'LC':\n",
    "        M = LogCholesky(dim = 2)\n",
    "        if not reference is None:\n",
    "            for index_to_plot in indices_to_plot:\n",
    "                sample = generate_random_spd_matrix(q_array=np.repeat(2, MC_samples), limits_unif = limits_unif, seed=4)\n",
    "                sample = [spd_to_log_chol(A) for A in sample]\n",
    "                for A in sample:\n",
    "                    if M.d(A, predictions[index_to_plot])<=Ralpha:\n",
    "                        plot_ellipse(log_chol_to_spd(A), ax=ax, xy=(index_to_plot/xy_factor,0), scale_factor=scale_factor, edgecolor=edge_color,\n",
    "                                    alpha=alpha)\n",
    "                        \n",
    "\n",
    "                plot_ellipse(log_chol_to_spd(predictions[index_to_plot]), ax=ax, \n",
    "                            xy=(index_to_plot/xy_factor,0), scale_factor=scale_factor, edgecolor='black', alpha=1)\n",
    "\n",
    "                plot_ellipse(log_chol_to_spd(reference[index_to_plot]), ax=ax, \n",
    "                            xy=(index_to_plot/xy_factor,0), scale_factor=scale_factor, edgecolor='red', alpha=1)\n",
    "\n",
    "        else:\n",
    "            for index_to_plot in indices_to_plot:\n",
    "                sample = generate_random_spd_matrix(q_array=np.repeat(df, MC_samples), limits_unif = limits_unif, seed=4)\n",
    "                for A in sample:\n",
    "                    if M.d(A, predictions[index_to_plot])<=Ralpha:\n",
    "\n",
    "                        plot_ellipse(log_chol_to_spd(A), ax=ax, xy=(index_to_plot/xy_factor,0), scale_factor=scale_factor, edgecolor=edge_color,\n",
    "                                    alpha=alpha)\n",
    "                                    \n",
    "\n",
    "                plot_ellipse(log_chol_to_spd(predictions[index_to_plot]), ax=ax, \n",
    "                            xy=(index_to_plot/xy_factor,0), scale_factor=scale_factor, edgecolor='black', alpha=1)\n",
    "            \n",
    "\n",
    "    elif dist == 'AI':\n",
    "        M = CustomAffineInvariant(dim = 2)\n",
    "        if not reference is None:\n",
    "            for index_to_plot in indices_to_plot:\n",
    "                sample = generate_random_spd_matrix(q_array=np.repeat(2, MC_samples), limits_unif = limits_unif, seed=4)\n",
    "                for A in sample:\n",
    "                    if M.d(A, predictions[index_to_plot])<=Ralpha:\n",
    "                        plot_ellipse(A, ax=ax, xy=(index_to_plot/xy_factor,0), scale_factor=scale_factor, edgecolor=edge_color,\n",
    "                                    alpha=alpha)\n",
    "                        \n",
    "\n",
    "                plot_ellipse(predictions[index_to_plot], ax=ax, \n",
    "                            xy=(index_to_plot/xy_factor,0), scale_factor=scale_factor, edgecolor='black', alpha=1)\n",
    "\n",
    "                plot_ellipse(reference[index_to_plot], ax=ax, \n",
    "                            xy=(index_to_plot/xy_factor,0), scale_factor=scale_factor, edgecolor='red', alpha=1)\n",
    "\n",
    "        else:\n",
    "            for index_to_plot in indices_to_plot:\n",
    "                sample = generate_random_spd_matrix(q_array=np.repeat(df, MC_samples), limits_unif = limits_unif, seed=4)\n",
    "    \n",
    "                for A in sample:\n",
    "                    if M.d(A, predictions[index_to_plot])<=Ralpha:\n",
    "\n",
    "                        plot_ellipse(A, ax=ax, xy=(index_to_plot/xy_factor,0), scale_factor=scale_factor, edgecolor=edge_color,\n",
    "                                    alpha=alpha)\n",
    "                                    \n",
    "\n",
    "                plot_ellipse(predictions[index_to_plot], ax=ax, \n",
    "                            xy=(index_to_plot/xy_factor,0), scale_factor=scale_factor, edgecolor='black', alpha=1)\n",
    "    else:\n",
    "        M = LogEuclidean(dim = 2)\n",
    "        if not reference is None:\n",
    "            for index_to_plot in indices_to_plot:\n",
    "                sample = generate_random_spd_matrix(q_array=np.repeat(2, MC_samples), limits_unif = limits_unif, seed=4)\n",
    "                for A in sample:\n",
    "\n",
    "                    if M.d(A, predictions[index_to_plot])<=Ralpha:\n",
    "                        plot_ellipse(A, ax=ax, xy=(index_to_plot/xy_factor,0), scale_factor=scale_factor, edgecolor=edge_color,\n",
    "                                    alpha=alpha)\n",
    "                        \n",
    "\n",
    "                plot_ellipse(predictions[index_to_plot], ax=ax, \n",
    "                            xy=(index_to_plot/xy_factor,0), scale_factor=scale_factor, edgecolor='black', alpha=1)\n",
    "\n",
    "                plot_ellipse(reference[index_to_plot], ax=ax, \n",
    "                            xy=(index_to_plot/xy_factor,0), scale_factor=scale_factor, edgecolor='red', alpha=1)\n",
    "\n",
    "        else:\n",
    "            for index_to_plot in indices_to_plot:\n",
    "                sample = generate_random_spd_matrix(q_array=np.repeat(df, MC_samples), limits_unif = limits_unif, seed=4)\n",
    "                for A in sample:\n",
    "                    if M.d(A, predictions[index_to_plot])<=Ralpha:\n",
    "\n",
    "                        plot_ellipse(A, ax=ax, xy=(index_to_plot/xy_factor,0), scale_factor=scale_factor, edgecolor=edge_color,\n",
    "                                    alpha=alpha)\n",
    "                                    \n",
    "\n",
    "                plot_ellipse(predictions[index_to_plot], ax=ax, \n",
    "                            xy=(index_to_plot/xy_factor,0), scale_factor=scale_factor, edgecolor='black', alpha=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_names = [2, 2.5, 3, 3.5, 4, 5, 6]\n",
    "\n",
    "# Obtain coverage results dataframe from the results files\n",
    "def coverage_results(dfs: list, dist: str= 'LC') -> pd.DataFrame:\n",
    "    coverage_df=pd.DataFrame(columns=['sample_index', 'train_size', 'df', 'y_train_data', 'train_predictions', 'OOB_quantile', 'OOB_errors', 'forest'])\n",
    "    for file in os.listdir(os.path.join(os.getcwd(), 'results')):\n",
    "        if file.endswith('.npy') and file.split('_')[0] == dist:\n",
    "            infile=open(os.path.join(os.getcwd(), 'results/'+file), 'rb')\n",
    "            result=np.load(infile, allow_pickle=True).item()\n",
    "            infile.close()\n",
    "            coverage_df=pd.concat([coverage_df, \n",
    "                                    pd.DataFrame({  'distance': dist,\n",
    "                                                    'sample_index': int(file.split('_')[2][4:]),\n",
    "                                                    'train_size': int(file.split('_')[3][1:]),\n",
    "                                                    'df': dfs_names[int(file.split('_')[4][2:])-1],\n",
    "                                                    'y_train_data': [result['y_train_data']],\n",
    "                                                    'train_predictions': [result['train_predictions']],\n",
    "                                                    'OOB_quantile': [result['OOB_quantile']],\n",
    "                                                    'OOB_errors': [result['OOB_errors']], \n",
    "                                                    'forest': [result['forest']],\n",
    "                                                }, index=pd.RangeIndex(0,1))],\n",
    "                                    ignore_index=True)\n",
    "        \n",
    "\n",
    "    coverage_df['train_size']=coverage_df['train_size'].astype('category')\n",
    "    coverage_df['sample_index']=coverage_df['sample_index'].astype('category')\n",
    "    coverage_df['df'] = coverage_df.df.astype('category')\n",
    "    return coverage_df\n",
    "\n",
    "coverage_df_AI=coverage_results(dist = 'AI', dfs=dfs_names)\n",
    "# coverage_df_LC=coverage_results(dist = 'LC', dfs=dfs_names)\n",
    "# coverage_df_LE=coverage_results(dist = 'LE', dfs=dfs_names)\n",
    "# \n",
    "# coverage_df_combined = pd.concat([coverage_df_AI, coverage_df_LC, coverage_df_LE], ignore_index=True)\n",
    "# print(coverage_df_AI.info())\n",
    "# print(coverage_df_LC.info())\n",
    "# print(coverage_df_LE.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 20\n",
    "n_estimations = 25\n",
    "\n",
    "zeros_init = np.zeros(shape = (n_estimations, 3))\n",
    "cov = np.zeros(shape = (n_estimations, 3))\n",
    "\n",
    "diccionario = {'df_2': {'AI': {'50': zeros_init, '100': zeros_init, '200': zeros_init, '500': zeros_init}, 'LC': {'50': zeros_init, '100': zeros_init, '200': zeros_init, '500': zeros_init}, 'LE': {'50': zeros_init, '100': zeros_init, '200': zeros_init, '500': zeros_init}}, 'df_5': {'AI': {'50': zeros_init, '100': zeros_init, '200': zeros_init, '500': zeros_init}, 'LC': {'50': zeros_init, '100': zeros_init, '200': zeros_init, '500': zeros_init}, 'LE': {'50': zeros_init, '100': zeros_init, '200': zeros_init, '500': zeros_init}}}\n",
    "\n",
    "# Obtain 25 estimations of Type I coverage error for each distance and N, to calculate the mean of the estimations and the sample variance\n",
    "for df in [2, 5]:\n",
    "    for dist in ['AI']:\n",
    "        # Select the distance analyzed\n",
    "        #if dist == 'AI':\n",
    "        coverage_df = coverage_df_AI[coverage_df_AI['df'] == df]\n",
    "        M = CustomAffineInvariant(dim = 2)\n",
    "        # elif dist == 'LC':\n",
    "        #     coverage_df = coverage_df_LC[coverage_df_LC['df'] == df]\n",
    "        #     M = LogCholesky(dim = 2)\n",
    "        # else:\n",
    "        #     coverage_df = coverage_df_LE[coverage_df_LE['df'] == df]\n",
    "        #     M = LogEuclidean(dim = 2)\n",
    "\n",
    "        for N in [50, 100, 200, 500]:\n",
    "            # Select the size of the training set\n",
    "            coverage_df_N = coverage_df[coverage_df['train_size'] == N]\n",
    "\n",
    "            for estimation in range(n_estimations):\n",
    "                yesno = np.zeros(3)\n",
    "                new_ys = sim_regression_matrices(Sigmas = (Sigma_1, Sigma_2, Sigma_3), \n",
    "                                                t = np.repeat(0, m),  \n",
    "                                               df = df)\n",
    "                lns = coverage_df_N.sample(n=m, replace=False)\n",
    "                i = 0\n",
    "                for _, ln in lns.iterrows():\n",
    "                    # Randomly select a row from the dataframe\n",
    "                    \n",
    "                    # Retrieve the data           \n",
    "                    new_y = new_ys['y'][i]\n",
    "                    new_pred = ln['forest'].predict_matrix(np.array([0]).reshape(-1,1))\n",
    "\n",
    "                    # Store the selected values\n",
    "                    yesno = np.vstack((yesno, M.d(new_pred, new_y) <= ln['OOB_quantile']))\n",
    "                    i += 1\n",
    "                cov[estimation, :] = yesno[1:,:].sum(axis=0) / m\n",
    "                \n",
    "            diccionario['df_'+str(df)][dist][str(N)] = np.copy(cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain 25 estimations of Type I coverage error for each distance and N, to calculate the mean of the estimations and the sample variance\n",
    "#dist = 'LC'\n",
    "#\n",
    "#for df in [2, 5]:\n",
    "#        \n",
    "#    coverage_df = coverage_df_LC[coverage_df_LC['df'] == df]\n",
    "#    M = LogCholesky(dim = 2)\n",
    "#\n",
    "#    for N in [50, 100, 200, 500]:\n",
    "#        # Select the size of the training set\n",
    "#        coverage_df_N = coverage_df[coverage_df['train_size'] == N]\n",
    "#\n",
    "#        for estimation in range(n_estimations):\n",
    "#            yesno = np.zeros(3)\n",
    "#\n",
    "#            for _ in range(m):\n",
    "#                # Randomly select a row from the dataframe\n",
    "#                random_row = coverage_df_N.sample(n=1)\n",
    "#                \n",
    "#                # Retrieve the data\n",
    "#                c_dq = 2 * np.exp((1 / 2) * sum([digamma((df - i + 1) / 2) for i in range(1, 2 + 1)]))\n",
    "#                sigma_t = Sigma_t(0, (Sigma_1, Sigma_2, Sigma_3))[0]\n",
    "#                sample_Y = np.array([spd_to_log_chol(1/c_dq*wishart( df=df, scale = sigma_t ).rvs( size=1 ))])\n",
    "#                y_test_datum = MetricData(M, sample_Y)\n",
    "#                test_pred = MetricData(M, np.array([random_row['forest'].values[0].predict_matrix(np.array([0]).reshape(-1,1))]))\n",
    "#                \n",
    "#                # Store the selected values\n",
    "#                yesno = np.vstack((yesno, M.d(test_pred, y_test_datum) <= random_row['OOB_quantile'].values[0]))\n",
    "#            cov[estimation, :] = yesno[1:,:].sum(axis=0) / m\n",
    "#            \n",
    "#        diccionario['df_'+str(df)][dist][str(N)] = np.copy(cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 degrees of freedom, N = 50, AI distance, mean of Type I coverage estimates:  [0.986 0.95  0.888]\n",
      "2 degrees of freedom, N = 50, AI distance, standard deviation of Type I coverage estimates:  [0.026533   0.05656854 0.07652451]\n",
      "2 degrees of freedom, N = 100, AI distance, mean of Type I coverage estimates:  [0.992 0.952 0.9  ]\n",
      "2 degrees of freedom, N = 100, AI distance, standard deviation of Type I coverage estimates:  [0.0183303  0.04118252 0.06480741]\n",
      "2 degrees of freedom, N = 200, AI distance, mean of Type I coverage estimates:  [0.98  0.956 0.91 ]\n",
      "2 degrees of freedom, N = 200, AI distance, standard deviation of Type I coverage estimates:  [0.03741657 0.06053098 0.07071068]\n",
      "2 degrees of freedom, N = 500, AI distance, mean of Type I coverage estimates:  [0.99  0.958 0.912]\n",
      "2 degrees of freedom, N = 500, AI distance, standard deviation of Type I coverage estimates:  [0.02       0.03655133 0.05706137]\n",
      "5 degrees of freedom, N = 50, AI distance, mean of Type I coverage estimates:  [0.98  0.95  0.896]\n",
      "5 degrees of freedom, N = 50, AI distance, standard deviation of Type I coverage estimates:  [0.03162278 0.04242641 0.05276362]\n",
      "5 degrees of freedom, N = 100, AI distance, mean of Type I coverage estimates:  [0.982 0.94  0.896]\n",
      "5 degrees of freedom, N = 100, AI distance, standard deviation of Type I coverage estimates:  [0.02785678 0.06       0.072     ]\n",
      "5 degrees of freedom, N = 200, AI distance, mean of Type I coverage estimates:  [0.984 0.948 0.924]\n",
      "5 degrees of freedom, N = 200, AI distance, standard deviation of Type I coverage estimates:  [0.02332381 0.04118252 0.0471593 ]\n",
      "5 degrees of freedom, N = 500, AI distance, mean of Type I coverage estimates:  [0.982 0.944 0.882]\n",
      "5 degrees of freedom, N = 500, AI distance, standard deviation of Type I coverage estimates:  [0.02785678 0.05885576 0.06764614]\n"
     ]
    }
   ],
   "source": [
    "for df in [2, 5]:    \n",
    "    for dist in ['AI']:\n",
    "        for N in [50, 100, 200, 500]:\n",
    "            print(f\"{df} degrees of freedom, N = {N}, {dist} distance, mean of Type I coverage estimates: \", np.mean(diccionario['df_'+str(df)][dist][str(N)], axis = 0)) \n",
    "            print(f\"{df} degrees of freedom, N = {N}, {dist} distance, standard deviation of Type I coverage estimates: \", np.sqrt(np.var(diccionario['df_'+str(df)][dist][str(N)], axis = 0))  )    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Distance</th>\n",
       "      <th colspan=\"3\" halign=\"left\">AI</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Significance Level</th>\n",
       "      <th>0.01</th>\n",
       "      <th>0.05</th>\n",
       "      <th>0.1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>df</th>\n",
       "      <th>N</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">df=2</th>\n",
       "      <th>N=50</th>\n",
       "      <td>0.986 (0.027)</td>\n",
       "      <td>0.950 (0.057)</td>\n",
       "      <td>0.888 (0.077)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N=100</th>\n",
       "      <td>0.992 (0.018)</td>\n",
       "      <td>0.952 (0.041)</td>\n",
       "      <td>0.900 (0.065)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N=200</th>\n",
       "      <td>0.980 (0.037)</td>\n",
       "      <td>0.956 (0.061)</td>\n",
       "      <td>0.910 (0.071)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N=500</th>\n",
       "      <td>0.990 (0.020)</td>\n",
       "      <td>0.958 (0.037)</td>\n",
       "      <td>0.912 (0.057)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">df=5</th>\n",
       "      <th>N=50</th>\n",
       "      <td>0.980 (0.032)</td>\n",
       "      <td>0.950 (0.042)</td>\n",
       "      <td>0.896 (0.053)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N=100</th>\n",
       "      <td>0.982 (0.028)</td>\n",
       "      <td>0.940 (0.060)</td>\n",
       "      <td>0.896 (0.072)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N=200</th>\n",
       "      <td>0.984 (0.023)</td>\n",
       "      <td>0.948 (0.041)</td>\n",
       "      <td>0.924 (0.047)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N=500</th>\n",
       "      <td>0.982 (0.028)</td>\n",
       "      <td>0.944 (0.059)</td>\n",
       "      <td>0.882 (0.068)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Distance                       AI                              \n",
       "Significance Level           0.01           0.05            0.1\n",
       "df   N                                                         \n",
       "df=2 N=50           0.986 (0.027)  0.950 (0.057)  0.888 (0.077)\n",
       "     N=100          0.992 (0.018)  0.952 (0.041)  0.900 (0.065)\n",
       "     N=200          0.980 (0.037)  0.956 (0.061)  0.910 (0.071)\n",
       "     N=500          0.990 (0.020)  0.958 (0.037)  0.912 (0.057)\n",
       "df=5 N=50           0.980 (0.032)  0.950 (0.042)  0.896 (0.053)\n",
       "     N=100          0.982 (0.028)  0.940 (0.060)  0.896 (0.072)\n",
       "     N=200          0.984 (0.023)  0.948 (0.041)  0.924 (0.047)\n",
       "     N=500          0.982 (0.028)  0.944 (0.059)  0.882 (0.068)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare data for the DataFrame\n",
    "rows = []\n",
    "index = []\n",
    "\n",
    "for df in [2, 5]:\n",
    "    for N in [50, 100, 200, 500]:\n",
    "        row = []\n",
    "        for dist in ['AI']:\n",
    "            means = np.mean(diccionario[f'df_{df}'][dist][str(N)], axis=0)\n",
    "            stds = np.sqrt(np.var(diccionario[f'df_{df}'][dist][str(N)], axis=0))\n",
    "            # Format as \"mean (std)\"\n",
    "            formatted_values = [f\"{means[i]:.3f} ({stds[i]:.3f})\" for i in range(3)]\n",
    "            row.extend(formatted_values)\n",
    "        rows.append(row)\n",
    "        index.append((f\"df={df}\", f\"N={N}\"))\n",
    "\n",
    "# MultiIndex for rows and columns\n",
    "row_index = pd.MultiIndex.from_tuples(index, names=[\"df\", \"N\"])\n",
    "col_index = pd.MultiIndex.from_product(\n",
    "    [[\"AI\"], [\"0.01\", \"0.05\", \"0.1\"]],\n",
    "    names=[\"Distance\", \"Significance Level\"]\n",
    ")\n",
    "\n",
    "# Create the DataFrame\n",
    "df = pd.DataFrame(rows, index=row_index, columns=col_index)\n",
    "\n",
    "# Display the DataFrame\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{table}\n",
      "\\caption{Type I error coverage for different distances, degrees of freedom and sample sizes}\n",
      "\\label{tab:typeIerrorcoverage}\n",
      "\\begin{tabular}{lllll}\n",
      "\\toprule\n",
      " & Distance & \\multicolumn{3}{c}{AI} \\\\\n",
      " & Significance Level & 0.01 & 0.05 & 0.1 \\\\\n",
      "df & N &  &  &  \\\\\n",
      "\\midrule\n",
      "\\multirow[t]{4}{*}{\\textbf{df=2}} & \\textbf{N=50} & 0.986 (0.027) & 0.950 (0.057) & 0.888 (0.077) \\\\\n",
      "\\textbf{} & \\textbf{N=100} & 0.992 (0.018) & 0.952 (0.041) & 0.900 (0.065) \\\\\n",
      "\\textbf{} & \\textbf{N=200} & 0.980 (0.037) & 0.956 (0.061) & 0.910 (0.071) \\\\\n",
      "\\textbf{} & \\textbf{N=500} & 0.990 (0.020) & 0.958 (0.037) & 0.912 (0.057) \\\\\n",
      "\\cline{1-5}\n",
      "\\multirow[t]{4}{*}{\\textbf{df=5}} & \\textbf{N=50} & 0.980 (0.032) & 0.950 (0.042) & 0.896 (0.053) \\\\\n",
      "\\textbf{} & \\textbf{N=100} & 0.982 (0.028) & 0.940 (0.060) & 0.896 (0.072) \\\\\n",
      "\\textbf{} & \\textbf{N=200} & 0.984 (0.023) & 0.948 (0.041) & 0.924 (0.047) \\\\\n",
      "\\textbf{} & \\textbf{N=500} & 0.982 (0.028) & 0.944 (0.059) & 0.882 (0.068) \\\\\n",
      "\\cline{1-5}\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\\end{table}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/z_/bb464vm54_vcx10blg24_d7m0000gn/T/ipykernel_1518/3085215903.py:9: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(format_cell)\n"
     ]
    }
   ],
   "source": [
    "def format_cell(value):\n",
    "    mean, std = value.split(\" \")\n",
    "    mean = f\"{float(mean):.3f}\"\n",
    "    std = std.strip(\"()\")\n",
    "    std = f\"({float(std):.3f})\"\n",
    "    return f\"{mean} {std}\"\n",
    "\n",
    "# Apply formatting to all cells\n",
    "df = df.applymap(format_cell)\n",
    "\n",
    "latex = df.to_latex(index=True, multirow=True, multicolumn=True, multicolumn_format='c', bold_rows=True, float_format= \"%.3f\" , caption='Type I error coverage for different distances, degrees of freedom and sample sizes', label='tab:typeIerrorcoverage')\n",
    "print(latex)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pballs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
