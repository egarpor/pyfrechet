{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convergencia Tipo X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Using numpy backend\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import sys, os\n",
    "\n",
    "from matplotlib.patches import Ellipse\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import sys, os; sys.path.append(os.path.dirname(os.getcwd()))\n",
    "from pyfrechet.metric_spaces import MetricData, LogCholesky, spd_to_log_chol, log_chol_to_spd\n",
    "from pyfrechet.regression.bagged_regressor import BaggedRegressor\n",
    "from pyfrechet.regression.trees import Tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pyfrechet.metric_spaces import MetricData, LogEuclidean, CustomAffineInvariant, CustomLogEuclidean, AffineInvariant, LogCholesky, log_chol_to_spd, spd_to_log_chol\n",
    "\n",
    "from scipy.special import digamma\n",
    "from scipy.stats import wishart\n",
    "\n",
    "from typing import Union\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_spd_matrix(q_array, limits_unif = 30, seed = 1):\n",
    "    \"\"\"Generate a random q x q symmetric positive definite (SPD) matrix.\"\"\"\n",
    "    np.random.RandomState(seed)\n",
    "    \n",
    "    q_array = np.array(q_array, dtype = int)\n",
    "    # Ensure the matrices are symmetric positive definite\n",
    "    mat = [(np.random.rand(q_array[i], q_array[i])-1/2)*limits_unif for i in range(len(q_array))]\n",
    "    return [np.dot(mat[i], mat[i].T) for i in range(len(q_array))]\n",
    "\n",
    "# Define the matrices to interpolate\n",
    "Sigma_1 = np.array([[1, -0.6],\n",
    "                  [-0.6, 0.5]])\n",
    "Sigma_2 = np.array([[1, 0],\n",
    "                  [0, 1]])\n",
    "Sigma_3 = np.array([[0.5, 0.4],\n",
    "                  [0.4, 1]])\n",
    "\n",
    "Sigmas = (Sigma_1, Sigma_2, Sigma_3)\n",
    "\n",
    "def Sigma_t(t_array, Sigma_array):\n",
    "    \"\"\"Provides an array with the matrices given by a regression model that interpolates between four matrices.\"\"\"  \n",
    "    \"\"\"The regression starts with Sigma_1 and then goes to Sigma_2 and Sigma_3 and ends in Sigma_4.\"\"\"\n",
    "    \n",
    "    # Define time intervals for interpolation\n",
    "    t_array = np.array(t_array)\n",
    "    t_array = t_array[:, None, None]\n",
    "\n",
    "    # Return the interpolated matrices\n",
    "    return np.where(t_array < 0.5, np.cos(np.pi*t_array)**2 * Sigma_array[0] + (1 - np.cos(np.pi*(1-t_array))**2) * Sigma_array[1], 0) + np.where(t_array >= 0.5, (1 - np.cos(np.pi*t_array)**2) * Sigma_array[1] + np.cos(np.pi*(1-t_array))**2 * Sigma_array[2], 0)\n",
    "\n",
    "\n",
    "def sim_regression_matrices(Sigmas: tuple,\n",
    "                            t: np.array,\n",
    "                            df: int=2):\n",
    "    t = np.array(t)\n",
    "    \n",
    "    #Simulate the time for regression (sample_t) and the true time (true_t)\n",
    "    q = Sigmas[0].shape[0]\n",
    "\n",
    "    c_dq = 2 * np.exp((1 / q) * sum( digamma((df - np.arange(1, q + 1) + 1 ) / 2) ))\n",
    "    sigma_t = Sigma_t(t, Sigmas)\n",
    "    sample_Y = [wishart( df=df, scale = sigma_t[k] / c_dq ).rvs( size=1 ) for k in range(t.shape[0])]\n",
    "    return {'t': t, 'y': sample_Y}\n",
    "\n",
    "\n",
    "def plot_ellipse(mat: np.ndarray, ax, \n",
    "                 xy: tuple=(0,0),\n",
    "                 scale_factor=1,\n",
    "                 edgecolor='red',\n",
    "                 facecolor='None',\n",
    "                 linewidth=2,\n",
    "                 alpha=1):\n",
    "    eigenvalues, eigenvectors = np.linalg.eig(mat)\n",
    "    theta = np.degrees(np.arctan2(*eigenvectors[:, 0][::-1]))\n",
    "    ellipse = Ellipse(xy=xy,\n",
    "                  width=scale_factor*np.sqrt(eigenvalues[0]),\n",
    "                  height=scale_factor*np.sqrt(eigenvalues[1]),\n",
    "                  angle=theta,\n",
    "                  edgecolor=edgecolor,\n",
    "                  facecolor=facecolor,\n",
    "                  lw=linewidth,\n",
    "                  alpha=alpha)\n",
    "    ax.add_patch(ellipse)\n",
    "\n",
    "\n",
    "def plot_OOB_balls_SPD( predictions: np.ndarray,\n",
    "                        indices_to_plot: list[int],\n",
    "                        Ralpha: float,\n",
    "                        ax,\n",
    "                        alpha: float = 0.05,\n",
    "                        reference: Union[np.ndarray, None]=None,\n",
    "                        scale_factor: float=1/10,\n",
    "                        xy_factor: float=50,\n",
    "                        df: int=5,\n",
    "                        MC_samples: int=100,\n",
    "                        edge_color='deepskyblue',\n",
    "                        dist : str = 'LC',\n",
    "                        limits_unif : int = 30\n",
    "                        ) -> None:\n",
    "    index_to_plot = 1\n",
    "    if dist == 'LC':\n",
    "        M = LogCholesky(dim = 2)\n",
    "        if not reference is None:\n",
    "            for index_to_plot in indices_to_plot:\n",
    "                sample = generate_random_spd_matrix(q_array=np.repeat(2, MC_samples), limits_unif = limits_unif, seed=4)\n",
    "                sample = [spd_to_log_chol(A) for A in sample]\n",
    "                for A in sample:\n",
    "                    if M.d(A, predictions[index_to_plot])<=Ralpha:\n",
    "                        plot_ellipse(log_chol_to_spd(A), ax=ax, xy=(index_to_plot/xy_factor,0), scale_factor=scale_factor, edgecolor=edge_color,\n",
    "                                    alpha=alpha)\n",
    "                        \n",
    "\n",
    "                plot_ellipse(log_chol_to_spd(predictions[index_to_plot]), ax=ax, \n",
    "                            xy=(index_to_plot/xy_factor,0), scale_factor=scale_factor, edgecolor='black', alpha=1)\n",
    "\n",
    "                plot_ellipse(log_chol_to_spd(reference[index_to_plot]), ax=ax, \n",
    "                            xy=(index_to_plot/xy_factor,0), scale_factor=scale_factor, edgecolor='red', alpha=1)\n",
    "\n",
    "        else:\n",
    "            for index_to_plot in indices_to_plot:\n",
    "                sample = generate_random_spd_matrix(q_array=np.repeat(df, MC_samples), limits_unif = limits_unif, seed=4)\n",
    "                for A in sample:\n",
    "                    if M.d(A, predictions[index_to_plot])<=Ralpha:\n",
    "\n",
    "                        plot_ellipse(log_chol_to_spd(A), ax=ax, xy=(index_to_plot/xy_factor,0), scale_factor=scale_factor, edgecolor=edge_color,\n",
    "                                    alpha=alpha)\n",
    "                                    \n",
    "\n",
    "                plot_ellipse(log_chol_to_spd(predictions[index_to_plot]), ax=ax, \n",
    "                            xy=(index_to_plot/xy_factor,0), scale_factor=scale_factor, edgecolor='black', alpha=1)\n",
    "            \n",
    "\n",
    "    elif dist == 'AI':\n",
    "        M = CustomAffineInvariant(dim = 2)\n",
    "        if not reference is None:\n",
    "            for index_to_plot in indices_to_plot:\n",
    "                sample = generate_random_spd_matrix(q_array=np.repeat(2, MC_samples), limits_unif = limits_unif, seed=4)\n",
    "                for A in sample:\n",
    "                    if M.d(A, predictions[index_to_plot])<=Ralpha:\n",
    "                        plot_ellipse(A, ax=ax, xy=(index_to_plot/xy_factor,0), scale_factor=scale_factor, edgecolor=edge_color,\n",
    "                                    alpha=alpha)\n",
    "                        \n",
    "\n",
    "                plot_ellipse(predictions[index_to_plot], ax=ax, \n",
    "                            xy=(index_to_plot/xy_factor,0), scale_factor=scale_factor, edgecolor='black', alpha=1)\n",
    "\n",
    "                plot_ellipse(reference[index_to_plot], ax=ax, \n",
    "                            xy=(index_to_plot/xy_factor,0), scale_factor=scale_factor, edgecolor='red', alpha=1)\n",
    "\n",
    "        else:\n",
    "            for index_to_plot in indices_to_plot:\n",
    "                sample = generate_random_spd_matrix(q_array=np.repeat(df, MC_samples), limits_unif = limits_unif, seed=4)\n",
    "    \n",
    "                for A in sample:\n",
    "                    if M.d(A, predictions[index_to_plot])<=Ralpha:\n",
    "\n",
    "                        plot_ellipse(A, ax=ax, xy=(index_to_plot/xy_factor,0), scale_factor=scale_factor, edgecolor=edge_color,\n",
    "                                    alpha=alpha)\n",
    "                                    \n",
    "\n",
    "                plot_ellipse(predictions[index_to_plot], ax=ax, \n",
    "                            xy=(index_to_plot/xy_factor,0), scale_factor=scale_factor, edgecolor='black', alpha=1)\n",
    "    else:\n",
    "        M = LogEuclidean(dim = 2)\n",
    "        if not reference is None:\n",
    "            for index_to_plot in indices_to_plot:\n",
    "                sample = generate_random_spd_matrix(q_array=np.repeat(2, MC_samples), limits_unif = limits_unif, seed=4)\n",
    "                for A in sample:\n",
    "\n",
    "                    if M.d(A, predictions[index_to_plot])<=Ralpha:\n",
    "                        plot_ellipse(A, ax=ax, xy=(index_to_plot/xy_factor,0), scale_factor=scale_factor, edgecolor=edge_color,\n",
    "                                    alpha=alpha)\n",
    "                        \n",
    "\n",
    "                plot_ellipse(predictions[index_to_plot], ax=ax, \n",
    "                            xy=(index_to_plot/xy_factor,0), scale_factor=scale_factor, edgecolor='black', alpha=1)\n",
    "\n",
    "                plot_ellipse(reference[index_to_plot], ax=ax, \n",
    "                            xy=(index_to_plot/xy_factor,0), scale_factor=scale_factor, edgecolor='red', alpha=1)\n",
    "\n",
    "        else:\n",
    "            for index_to_plot in indices_to_plot:\n",
    "                sample = generate_random_spd_matrix(q_array=np.repeat(df, MC_samples), limits_unif = limits_unif, seed=4)\n",
    "                for A in sample:\n",
    "                    if M.d(A, predictions[index_to_plot])<=Ralpha:\n",
    "\n",
    "                        plot_ellipse(A, ax=ax, xy=(index_to_plot/xy_factor,0), scale_factor=scale_factor, edgecolor=edge_color,\n",
    "                                    alpha=alpha)\n",
    "                                    \n",
    "\n",
    "                plot_ellipse(predictions[index_to_plot], ax=ax, \n",
    "                            xy=(index_to_plot/xy_factor,0), scale_factor=scale_factor, edgecolor='black', alpha=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_names = [2, 2.5, 3, 3.5, 4, 5, 6]\n",
    "\n",
    "# Obtain coverage results dataframe from the results files\n",
    "def coverage_results(dfs: list, dist: str= 'LC') -> pd.DataFrame:\n",
    "    coverage_df=pd.DataFrame(columns=['sample_index', 'train_size', 'df', 'y_train_data', 'train_predictions', 'OOB_quantile', 'OOB_errors', 'forest'])\n",
    "    for file in os.listdir(os.path.join(os.getcwd(), 'results')):\n",
    "        if file.endswith('.npy') and file.split('_')[0] == dist:\n",
    "            infile=open(os.path.join(os.getcwd(), 'results/' + file), 'rb')\n",
    "            result=np.load(infile, allow_pickle=True).item()\n",
    "            infile.close()\n",
    "            coverage_df=pd.concat([coverage_df, \n",
    "                                    pd.DataFrame({  'distance': dist,\n",
    "                                                    'sample_index': int(file.split('_')[2][4:]),\n",
    "                                                    'train_size': int(file.split('_')[3][1:]),\n",
    "                                                    'df': dfs[int(file.split('_')[4][2:])-1],\n",
    "                                                    'y_train_data': [result['y_train_data']],\n",
    "                                                    'train_predictions': [result['train_predictions']],\n",
    "                                                    'OOB_quantile': [result['OOB_quantile']],\n",
    "                                                    'OOB_errors': [result['OOB_errors']], \n",
    "                                                    'forest': [result['forest']],\n",
    "                                                }, index=pd.RangeIndex(0,1))],\n",
    "                                    ignore_index=True)\n",
    "        \n",
    "    coverage_df['train_size']=coverage_df['train_size'].astype('category')\n",
    "    coverage_df['sample_index']=coverage_df['sample_index'].astype('category')\n",
    "    coverage_df['df'] = coverage_df.df.astype('category')\n",
    "    return coverage_df\n",
    "\n",
    "coverage_df_AI=coverage_results(dfs = dfs_names, dist = 'AI')\n",
    "#coverage_df_LC=coverage_results(dfs = dfs_names, dist = 'LC')\n",
    "#coverage_df_LE=coverage_results(dfs = dfs_names, dist = 'LE')\n",
    "#\n",
    "#coverage_df_combined = pd.concat([coverage_df_AI, coverage_df_LC, coverage_df_LE], ignore_index=True)\n",
    "#print(coverage_df_AI.info())\n",
    "#print(coverage_df_LC.info())\n",
    "#print(coverage_df_LE.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 20\n",
    "n_estimations = 10\n",
    "\n",
    "zeros_init = np.zeros(shape = (n_estimations, 3))\n",
    "cov = np.zeros(shape = (n_estimations, 3))\n",
    "\n",
    "diccionario = {\n",
    "     'df_5': {'AI': {'50': zeros_init, '100': zeros_init, '200': zeros_init, '500': zeros_init}, 'LC': {'50': zeros_init, '100': zeros_init, '200': zeros_init, '500': zeros_init}, 'LE': {'50': zeros_init, '100': zeros_init, '200': zeros_init, '500': zeros_init}}}\n",
    "\n",
    "# Obtain 25 estimations of Type I coverage error for each distance and N, to calculate the mean of the estimations and the sample variance\n",
    "for df in [5]:\n",
    "    for dist in ['AI']:\n",
    "        # Select the distance analyzed\n",
    "        #if dist == 'AI':\n",
    "        #    coverage_df = coverage_df_AI[coverage_df_AI['df'] == df]\n",
    "        #    M = CustomAffineInvariant(dim = 2)\n",
    "        #elif dist == 'LC':\n",
    "        #    coverage_df = coverage_df_LC[coverage_df_LC['df'] == df]\n",
    "        #    M = LogCholesky(dim = 2)\n",
    "        #else\n",
    "        #    coverage_df = coverage_df_LE[coverage_df_LE['df'] == df]\n",
    "        #    M = LogEuclidean(dim = 2)\n",
    "\n",
    "        coverage_df = coverage_df_AI[coverage_df_AI['df'] == df]\n",
    "        M = CustomAffineInvariant(dim = 2)\n",
    "        \n",
    "        for N in [50, 100, 200, 500]:\n",
    "            # Select the size of the training set\n",
    "            coverage_df_N = coverage_df[coverage_df['train_size'] == N]\n",
    "            for estimation in range(n_estimations):\n",
    "                yesno = np.zeros(3)\n",
    "                # Randomly select rows from the dataframe\n",
    "                new_ts = np.random.uniform(size = m)\n",
    "                new_ys = sim_regression_matrices(Sigmas = (Sigma_1, Sigma_2, Sigma_3), \n",
    "                                                t = new_ts,  \n",
    "                                                df = df)\n",
    "                lns = coverage_df_N.sample(n=m, replace=False)\n",
    "        \n",
    "                i = 0\n",
    "                for _, ln in lns.iterrows():\n",
    "                    # Generate one random point to test if it belongs to the prediction ball\n",
    "                    new_t = new_ts[i]\n",
    "                    #new_t = np.random.uniform(size = 1)\n",
    "                    #Predict the new observation\n",
    "                    new_pred = ln['forest'].predict_matrix(new_t.reshape(-1,1))\n",
    "                    new_y = new_ys['y'][i]\n",
    "                    #new_y = sim_regression_matrices(Sigmas = (Sigma_1, Sigma_2, Sigma_3), \n",
    "                    #            t = new_t,  \n",
    "                    #            df = df)['y'][0]\n",
    "                    # Store the selected values\n",
    "                    yesno = np.vstack((yesno, M.d(new_pred, new_y) <= ln['OOB_quantile']))\n",
    "                    i += 1\n",
    "                cov[estimation, :] = yesno[1:,:].sum(axis=0) / m\n",
    "                \n",
    "            diccionario['df_'+str(df)][dist][str(N)] = np.copy(cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Maximum number of iterations 32 reached. The mean may be inaccurate\n",
      "WARNING: Maximum number of iterations 32 reached. The mean may be inaccurate\n",
      "WARNING: Maximum number of iterations 32 reached. The mean may be inaccurate\n",
      "WARNING: Maximum number of iterations 32 reached. The mean may be inaccurate\n",
      "WARNING: Maximum number of iterations 32 reached. The mean may be inaccurate\n",
      "WARNING: Maximum number of iterations 32 reached. The mean may be inaccurate\n",
      "WARNING: Maximum number of iterations 32 reached. The mean may be inaccurate\n",
      "WARNING: Maximum number of iterations 32 reached. The mean may be inaccurate\n",
      "WARNING: Maximum number of iterations 32 reached. The mean may be inaccurate\n",
      "WARNING: Maximum number of iterations 32 reached. The mean may be inaccurate\n",
      "WARNING: Maximum number of iterations 32 reached. The mean may be inaccurate\n",
      "WARNING: Maximum number of iterations 32 reached. The mean may be inaccurate\n",
      "WARNING: Maximum number of iterations 32 reached. The mean may be inaccurate\n",
      "WARNING: Maximum number of iterations 32 reached. The mean may be inaccurate\n"
     ]
    }
   ],
   "source": [
    "m = 20\n",
    "n_estimations = 10\n",
    "\n",
    "zeros_init = np.zeros(shape = (n_estimations, 3))\n",
    "cov = np.zeros(shape = (n_estimations, 3))\n",
    "\n",
    "diccionario = {'df_2': {'AI': {'50': zeros_init, '100': zeros_init, '200': zeros_init, '500': zeros_init}, 'LC': {'50': zeros_init, '100': zeros_init, '200': zeros_init, '500': zeros_init}, 'LE': {'50': zeros_init, '100': zeros_init, '200': zeros_init, '500': zeros_init}},  'df_5': {'AI': {'50': zeros_init, '100': zeros_init, '200': zeros_init, '500': zeros_init}, 'LC': {'50': zeros_init, '100': zeros_init, '200': zeros_init, '500': zeros_init}, 'LE': {'50': zeros_init, '100': zeros_init, '200': zeros_init, '500': zeros_init}}}\n",
    "\n",
    "# Obtain 25 estimations of Type I coverage error for each distance and N, to calculate the mean of the estimations and the sample variance\n",
    "for df in [2, 5]:\n",
    "    for dist in ['AI']:\n",
    "        # Select the distance analyzed\n",
    "        #if dist == 'AI':\n",
    "        #    coverage_df = coverage_df_AI[coverage_df_AI['df'] == df]\n",
    "        #    M = CustomAffineInvariant(dim = 2)\n",
    "        #elif dist == 'LC':\n",
    "        #    coverage_df = coverage_df_LC[coverage_df_LC['df'] == df]\n",
    "        #    M = LogCholesky(dim = 2)\n",
    "        #else:\n",
    "        #    coverage_df = coverage_df_LE[coverage_df_LE['df'] == df]\n",
    "        #    M = LogEuclidean(dim = 2)\n",
    "\n",
    "        coverage_df = coverage_df_AI[coverage_df_AI['df'] == df]\n",
    "        M = CustomAffineInvariant(dim = 2)\n",
    "        \n",
    "        for N in [50, 100, 200, 500]:\n",
    "            # Select the size of the training set\n",
    "            coverage_df_N = coverage_df[coverage_df['train_size'] == N]\n",
    "            for estimation in range(n_estimations):\n",
    "                yesno = np.zeros(3)\n",
    "                # Randomly select rows from the dataframe\n",
    "                #new_ts = np.random.uniform(size = m)\n",
    "                #new_ys = sim_regression_matrices(Sigmas = (Sigma_1, Sigma_2, Sigma_3), \n",
    "                #                                t = new_ts,  \n",
    "                #                                df = df)\n",
    "                lns = coverage_df_N.sample(n=m, replace=False)\n",
    "        \n",
    "                i = 0\n",
    "                for _, ln in lns.iterrows():\n",
    "                    # Generate one random point to test if it belongs to the prediction ball\n",
    "                    new_t = np.random.uniform(size = 1)\n",
    "                    #Predict the new observation\n",
    "                    new_pred = ln['forest'].predict_matrix(new_t.reshape(-1,1))\n",
    "                    #new_y = new_ys['y'][i]\n",
    "                    new_y = sim_regression_matrices(Sigmas = (Sigma_1, Sigma_2, Sigma_3), \n",
    "                                t = new_t,  \n",
    "                                df = df)['y'][0]\n",
    "                    # Store the selected values\n",
    "                    yesno = np.vstack((yesno, M.d(new_pred, new_y) <= ln['OOB_quantile']))\n",
    "                    i += 1\n",
    "                cov[estimation, :] = yesno[1:,:].sum(axis=0) / m\n",
    "                \n",
    "            diccionario['df_'+str(df)][dist][str(N)] = np.copy(cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 degrees of freedom, N = 50, AI distance, mean of Type I coverage estimates:  [0.995 0.94  0.895]\n",
      "2 degrees of freedom, N = 50, AI distance, standard deviation of Type I coverage estimates:  [0.015      0.05385165 0.07566373]\n",
      "2 degrees of freedom, N = 100, AI distance, mean of Type I coverage estimates:  [0.985 0.96  0.93 ]\n",
      "2 degrees of freedom, N = 100, AI distance, standard deviation of Type I coverage estimates:  [0.02291288 0.04358899 0.04      ]\n",
      "2 degrees of freedom, N = 200, AI distance, mean of Type I coverage estimates:  [0.99  0.95  0.905]\n",
      "2 degrees of freedom, N = 200, AI distance, standard deviation of Type I coverage estimates:  [0.02       0.03872983 0.04153312]\n",
      "2 degrees of freedom, N = 500, AI distance, mean of Type I coverage estimates:  [0.995 0.95  0.885]\n",
      "2 degrees of freedom, N = 500, AI distance, standard deviation of Type I coverage estimates:  [0.015      0.03162278 0.06344289]\n",
      "5 degrees of freedom, N = 50, AI distance, mean of Type I coverage estimates:  [0.97  0.925 0.91 ]\n",
      "5 degrees of freedom, N = 50, AI distance, standard deviation of Type I coverage estimates:  [0.03316625 0.0559017  0.07      ]\n",
      "5 degrees of freedom, N = 100, AI distance, mean of Type I coverage estimates:  [0.985 0.945 0.89 ]\n",
      "5 degrees of freedom, N = 100, AI distance, standard deviation of Type I coverage estimates:  [0.03201562 0.05220153 0.07348469]\n",
      "5 degrees of freedom, N = 200, AI distance, mean of Type I coverage estimates:  [0.965 0.95  0.88 ]\n",
      "5 degrees of freedom, N = 200, AI distance, standard deviation of Type I coverage estimates:  [0.03905125 0.03872983 0.0509902 ]\n",
      "5 degrees of freedom, N = 500, AI distance, mean of Type I coverage estimates:  [0.995 0.975 0.925]\n",
      "5 degrees of freedom, N = 500, AI distance, standard deviation of Type I coverage estimates:  [0.015      0.03354102 0.06800735]\n"
     ]
    }
   ],
   "source": [
    "for df in [2, 5]:    \n",
    "    for dist in ['AI']:\n",
    "        for N in [50, 100, 200, 500]:\n",
    "            print(f\"{df} degrees of freedom, N = {N}, {dist} distance, mean of Type I coverage estimates: \", np.mean(diccionario['df_'+str(df)][dist][str(N)], axis = 0)) \n",
    "            print(f\"{df} degrees of freedom, N = {N}, {dist} distance, standard deviation of Type I coverage estimates: \", np.sqrt(np.var(diccionario['df_'+str(df)][dist][str(N)], axis = 0))  )    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Distance</th>\n",
       "      <th colspan=\"3\" halign=\"left\">AI</th>\n",
       "      <th colspan=\"3\" halign=\"left\">LC</th>\n",
       "      <th colspan=\"3\" halign=\"left\">LE</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Significance Level</th>\n",
       "      <th>0.01</th>\n",
       "      <th>0.05</th>\n",
       "      <th>0.1</th>\n",
       "      <th>0.01</th>\n",
       "      <th>0.05</th>\n",
       "      <th>0.1</th>\n",
       "      <th>0.01</th>\n",
       "      <th>0.05</th>\n",
       "      <th>0.1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>df</th>\n",
       "      <th>N</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">df=2</th>\n",
       "      <th>N=50</th>\n",
       "      <td>0.9950 (0.0150)</td>\n",
       "      <td>0.9400 (0.0539)</td>\n",
       "      <td>0.8950 (0.0757)</td>\n",
       "      <td>0.0000 (0.0000)</td>\n",
       "      <td>0.0000 (0.0000)</td>\n",
       "      <td>0.0000 (0.0000)</td>\n",
       "      <td>0.0000 (0.0000)</td>\n",
       "      <td>0.0000 (0.0000)</td>\n",
       "      <td>0.0000 (0.0000)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N=100</th>\n",
       "      <td>0.9850 (0.0229)</td>\n",
       "      <td>0.9600 (0.0436)</td>\n",
       "      <td>0.9300 (0.0400)</td>\n",
       "      <td>0.0000 (0.0000)</td>\n",
       "      <td>0.0000 (0.0000)</td>\n",
       "      <td>0.0000 (0.0000)</td>\n",
       "      <td>0.0000 (0.0000)</td>\n",
       "      <td>0.0000 (0.0000)</td>\n",
       "      <td>0.0000 (0.0000)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N=200</th>\n",
       "      <td>0.9900 (0.0200)</td>\n",
       "      <td>0.9500 (0.0387)</td>\n",
       "      <td>0.9050 (0.0415)</td>\n",
       "      <td>0.0000 (0.0000)</td>\n",
       "      <td>0.0000 (0.0000)</td>\n",
       "      <td>0.0000 (0.0000)</td>\n",
       "      <td>0.0000 (0.0000)</td>\n",
       "      <td>0.0000 (0.0000)</td>\n",
       "      <td>0.0000 (0.0000)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N=500</th>\n",
       "      <td>0.9950 (0.0150)</td>\n",
       "      <td>0.9500 (0.0316)</td>\n",
       "      <td>0.8850 (0.0634)</td>\n",
       "      <td>0.0000 (0.0000)</td>\n",
       "      <td>0.0000 (0.0000)</td>\n",
       "      <td>0.0000 (0.0000)</td>\n",
       "      <td>0.0000 (0.0000)</td>\n",
       "      <td>0.0000 (0.0000)</td>\n",
       "      <td>0.0000 (0.0000)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">df=5</th>\n",
       "      <th>N=50</th>\n",
       "      <td>0.9700 (0.0332)</td>\n",
       "      <td>0.9250 (0.0559)</td>\n",
       "      <td>0.9100 (0.0700)</td>\n",
       "      <td>0.0000 (0.0000)</td>\n",
       "      <td>0.0000 (0.0000)</td>\n",
       "      <td>0.0000 (0.0000)</td>\n",
       "      <td>0.0000 (0.0000)</td>\n",
       "      <td>0.0000 (0.0000)</td>\n",
       "      <td>0.0000 (0.0000)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N=100</th>\n",
       "      <td>0.9850 (0.0320)</td>\n",
       "      <td>0.9450 (0.0522)</td>\n",
       "      <td>0.8900 (0.0735)</td>\n",
       "      <td>0.0000 (0.0000)</td>\n",
       "      <td>0.0000 (0.0000)</td>\n",
       "      <td>0.0000 (0.0000)</td>\n",
       "      <td>0.0000 (0.0000)</td>\n",
       "      <td>0.0000 (0.0000)</td>\n",
       "      <td>0.0000 (0.0000)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N=200</th>\n",
       "      <td>0.9650 (0.0391)</td>\n",
       "      <td>0.9500 (0.0387)</td>\n",
       "      <td>0.8800 (0.0510)</td>\n",
       "      <td>0.0000 (0.0000)</td>\n",
       "      <td>0.0000 (0.0000)</td>\n",
       "      <td>0.0000 (0.0000)</td>\n",
       "      <td>0.0000 (0.0000)</td>\n",
       "      <td>0.0000 (0.0000)</td>\n",
       "      <td>0.0000 (0.0000)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N=500</th>\n",
       "      <td>0.9950 (0.0150)</td>\n",
       "      <td>0.9750 (0.0335)</td>\n",
       "      <td>0.9250 (0.0680)</td>\n",
       "      <td>0.0000 (0.0000)</td>\n",
       "      <td>0.0000 (0.0000)</td>\n",
       "      <td>0.0000 (0.0000)</td>\n",
       "      <td>0.0000 (0.0000)</td>\n",
       "      <td>0.0000 (0.0000)</td>\n",
       "      <td>0.0000 (0.0000)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Distance                         AI                                    \\\n",
       "Significance Level             0.01             0.05              0.1   \n",
       "df   N                                                                  \n",
       "df=2 N=50           0.9950 (0.0150)  0.9400 (0.0539)  0.8950 (0.0757)   \n",
       "     N=100          0.9850 (0.0229)  0.9600 (0.0436)  0.9300 (0.0400)   \n",
       "     N=200          0.9900 (0.0200)  0.9500 (0.0387)  0.9050 (0.0415)   \n",
       "     N=500          0.9950 (0.0150)  0.9500 (0.0316)  0.8850 (0.0634)   \n",
       "df=5 N=50           0.9700 (0.0332)  0.9250 (0.0559)  0.9100 (0.0700)   \n",
       "     N=100          0.9850 (0.0320)  0.9450 (0.0522)  0.8900 (0.0735)   \n",
       "     N=200          0.9650 (0.0391)  0.9500 (0.0387)  0.8800 (0.0510)   \n",
       "     N=500          0.9950 (0.0150)  0.9750 (0.0335)  0.9250 (0.0680)   \n",
       "\n",
       "Distance                         LC                                    \\\n",
       "Significance Level             0.01             0.05              0.1   \n",
       "df   N                                                                  \n",
       "df=2 N=50           0.0000 (0.0000)  0.0000 (0.0000)  0.0000 (0.0000)   \n",
       "     N=100          0.0000 (0.0000)  0.0000 (0.0000)  0.0000 (0.0000)   \n",
       "     N=200          0.0000 (0.0000)  0.0000 (0.0000)  0.0000 (0.0000)   \n",
       "     N=500          0.0000 (0.0000)  0.0000 (0.0000)  0.0000 (0.0000)   \n",
       "df=5 N=50           0.0000 (0.0000)  0.0000 (0.0000)  0.0000 (0.0000)   \n",
       "     N=100          0.0000 (0.0000)  0.0000 (0.0000)  0.0000 (0.0000)   \n",
       "     N=200          0.0000 (0.0000)  0.0000 (0.0000)  0.0000 (0.0000)   \n",
       "     N=500          0.0000 (0.0000)  0.0000 (0.0000)  0.0000 (0.0000)   \n",
       "\n",
       "Distance                         LE                                    \n",
       "Significance Level             0.01             0.05              0.1  \n",
       "df   N                                                                 \n",
       "df=2 N=50           0.0000 (0.0000)  0.0000 (0.0000)  0.0000 (0.0000)  \n",
       "     N=100          0.0000 (0.0000)  0.0000 (0.0000)  0.0000 (0.0000)  \n",
       "     N=200          0.0000 (0.0000)  0.0000 (0.0000)  0.0000 (0.0000)  \n",
       "     N=500          0.0000 (0.0000)  0.0000 (0.0000)  0.0000 (0.0000)  \n",
       "df=5 N=50           0.0000 (0.0000)  0.0000 (0.0000)  0.0000 (0.0000)  \n",
       "     N=100          0.0000 (0.0000)  0.0000 (0.0000)  0.0000 (0.0000)  \n",
       "     N=200          0.0000 (0.0000)  0.0000 (0.0000)  0.0000 (0.0000)  \n",
       "     N=500          0.0000 (0.0000)  0.0000 (0.0000)  0.0000 (0.0000)  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare data for the DataFrame\n",
    "rows = []\n",
    "index = []\n",
    "\n",
    "for df in [2, 5]:\n",
    "    for N in [50, 100, 200, 500]:\n",
    "        row = []\n",
    "        for dist in ['AI', 'LC', 'LE']:\n",
    "            means = np.mean(diccionario[f'df_{df}'][dist][str(N)], axis=0)\n",
    "            stds = np.sqrt(np.var(diccionario[f'df_{df}'][dist][str(N)], axis=0))\n",
    "            # Format as \"mean (std)\"\n",
    "            formatted_values = [f\"{means[i]:.4f} ({stds[i]:.4f})\" for i in range(3)]\n",
    "            row.extend(formatted_values)\n",
    "        rows.append(row)\n",
    "        index.append((f\"df={df}\", f\"N={N}\"))\n",
    "\n",
    "# MultiIndex for rows and columns\n",
    "row_index = pd.MultiIndex.from_tuples(index, names=[\"df\", \"N\"])\n",
    "col_index = pd.MultiIndex.from_product(\n",
    "    [[\"AI\", \"LC\", \"LE\"], [\"0.01\", \"0.05\", \"0.1\"]],\n",
    "    names=[\"Distance\", \"Significance Level\"]\n",
    ")\n",
    "\n",
    "# Create the DataFrame\n",
    "df = pd.DataFrame(rows, index=row_index, columns=col_index)\n",
    "\n",
    "# Display the DataFrame\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/z_/bb464vm54_vcx10blg24_d7m0000gn/T/ipykernel_4072/3715945492.py:10: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(format_cell)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "def format_cell(value):\n",
    "    mean, std = value.split(\" \")\n",
    "    mean = f\"{float(mean):.3f}\"\n",
    "    std = std.strip(\"()\")\n",
    "    std = f\"({float(std):.3f})\"\n",
    "    return f\"{mean} {std}\"\n",
    "\n",
    "# Apply formatting to all cells\n",
    "df = df.applymap(format_cell)\n",
    "\n",
    "latex = df.to_latex(index=True, multirow=True, multicolumn=True, multicolumn_format='c', bold_rows=True, float_format= \"%.3f\" , caption='Type I error coverage for different distances, degrees of freedom and sample sizes', label='tab:typeIerrorcoverage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{table}\n",
      "\\caption{Type I error coverage for different distances, degrees of freedom and sample sizes}\n",
      "\\label{tab:typeIerrorcoverage}\n",
      "\\begin{tabular}{lllllllllll}\n",
      "\\toprule\n",
      " & Distance & \\multicolumn{3}{c}{AI} & \\multicolumn{3}{c}{LC} & \\multicolumn{3}{c}{LE} \\\\\n",
      " & Significance Level & 0.01 & 0.05 & 0.1 & 0.01 & 0.05 & 0.1 & 0.01 & 0.05 & 0.1 \\\\\n",
      "df & N &  &  &  &  &  &  &  &  &  \\\\\n",
      "\\midrule\n",
      "\\multirow[t]{4}{*}{\\textbf{df=2}} & \\textbf{N=50} & 0.995 (0.015) & 0.940 (0.054) & 0.895 (0.076) & 0.000 (0.000) & 0.000 (0.000) & 0.000 (0.000) & 0.000 (0.000) & 0.000 (0.000) & 0.000 (0.000) \\\\\n",
      "\\textbf{} & \\textbf{N=100} & 0.985 (0.023) & 0.960 (0.044) & 0.930 (0.040) & 0.000 (0.000) & 0.000 (0.000) & 0.000 (0.000) & 0.000 (0.000) & 0.000 (0.000) & 0.000 (0.000) \\\\\n",
      "\\textbf{} & \\textbf{N=200} & 0.990 (0.020) & 0.950 (0.039) & 0.905 (0.042) & 0.000 (0.000) & 0.000 (0.000) & 0.000 (0.000) & 0.000 (0.000) & 0.000 (0.000) & 0.000 (0.000) \\\\\n",
      "\\textbf{} & \\textbf{N=500} & 0.995 (0.015) & 0.950 (0.032) & 0.885 (0.063) & 0.000 (0.000) & 0.000 (0.000) & 0.000 (0.000) & 0.000 (0.000) & 0.000 (0.000) & 0.000 (0.000) \\\\\n",
      "\\cline{1-11}\n",
      "\\multirow[t]{4}{*}{\\textbf{df=5}} & \\textbf{N=50} & 0.970 (0.033) & 0.925 (0.056) & 0.910 (0.070) & 0.000 (0.000) & 0.000 (0.000) & 0.000 (0.000) & 0.000 (0.000) & 0.000 (0.000) & 0.000 (0.000) \\\\\n",
      "\\textbf{} & \\textbf{N=100} & 0.985 (0.032) & 0.945 (0.052) & 0.890 (0.073) & 0.000 (0.000) & 0.000 (0.000) & 0.000 (0.000) & 0.000 (0.000) & 0.000 (0.000) & 0.000 (0.000) \\\\\n",
      "\\textbf{} & \\textbf{N=200} & 0.965 (0.039) & 0.950 (0.039) & 0.880 (0.051) & 0.000 (0.000) & 0.000 (0.000) & 0.000 (0.000) & 0.000 (0.000) & 0.000 (0.000) & 0.000 (0.000) \\\\\n",
      "\\textbf{} & \\textbf{N=500} & 0.995 (0.015) & 0.975 (0.034) & 0.925 (0.068) & 0.000 (0.000) & 0.000 (0.000) & 0.000 (0.000) & 0.000 (0.000) & 0.000 (0.000) & 0.000 (0.000) \\\\\n",
      "\\cline{1-11}\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\\end{table}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(latex)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pballs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
